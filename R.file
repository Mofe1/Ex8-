#=============Require sent for Preperation=========
require(tm)
require(tm.corpus.Reuters21578)
require(slam)
require(NLP)
require(openNLP)
require(openNLPdata)
require(rJava)
require(SnowballC)
require(plyr)
require(class)
require(RTextTools)
require(mclust)
require(fpc)
require(tm.corpus.Reuters21578)

library(tm)

#set tm options
options(stringsAsFactors=FALSE)

#loading Corpus-

require(tm.corpus.Reuters21578)
data(Reuters21578)
rt <-Reuters21578
rt[[1]]


ID(rt[[1]])                     
meta(rt[[1]])                   
show(rt)                        
summary(rt)                     
inspect(rt)  


#2. Preprocessing

prop.rt <- function(rt) {
    rtlower <- tm_map(rt, tolower)
    rtnum <- tm_map(rtlower, removeNumbers)
    rtpun <- tm_map(rtnum, removePunctuation)
    rtsw <- tm_map(rtpun, removeWords, stopwords("english"))
    rtownSW <- tm_map(rtsw, removeWords, c("", ""))
    rt <- tm_map(rt, removeWords, "said") 
    rt <- tm_map(rt, removeWords, "and") 
    rtWs <- tm_map(rtownSW, stripWhitespace)
    rt_Doc <- tm_map(rtWs,stemDocument)

  clean_corpus <- rt_Doc
  
  return(clean_corpus)
}

#prop.rt(rt)


#given topics
topics <- c("earn", "acquisitions", "money-fx", "grain", "crude", "trade", "interest", "ship", "wheat", "corn")

#function to generate tdms and matrices-Tokenization 

loadDtm <- function() {
  rtdtm <-DocumentTermMatrix(clean_corpus)
  rtdtm <- removeSparseTerms(rtdtm, 0.995)
}


loadDF <- function() {
  rtdf <- as.data.frame(inspect(rtdtm))
}


loadTopics <- function() {
  v1 <- data.frame()
  for (t in 1:length(topics)) {
    for(i in 1:length(clean.corpus)) {
      d[i,t] <- any(meta(clean_corpus[[i]],tag="Topics")==topics[t])
    }
  }
  colnames(d) <- headings
  v1 <- sapply(d,as.factor)
}




#3. Split Corpus into train set and test set
#Train Set

Splrt1 <- "LEWISSPLIT == 'TRAIN'"
rtTrain <- tm_filter(rt, FUN = sFilter, Splrt1)

#Test set
Splrt2 <- "LEWISSPLIT == 'TEST'"
rtTest <- tm_filter(rt, FUN = sFilter, Splrt2)
